{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "# a folder\n",
        "url = \"https://drive.google.com/drive/folders/1-3kWqNW55w5Qqh1nVEi8r8xbfaxGtLPZ?usp=drive_link\"\n",
        "gdown.download_folder(url, quiet=True, use_cookies=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKXcGwsEiaKm",
        "outputId": "f8ed9742-203e-420d-d000-2f1d707db629"
      },
      "id": "tKXcGwsEiaKm",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Undi95_Xwin-MLewd-13B-V0.2/added_tokens.json',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/config.json',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/huggingface-metadata.txt',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/model-00001-of-00003.safetensors',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/model-00002-of-00003.safetensors',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/model-00003-of-00003.safetensors',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/model.safetensors.index.json',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/README.md',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/special_tokens_map.json',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/tokenizer_config.json',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/tokenizer.json',\n",
              " '/content/Undi95_Xwin-MLewd-13B-V0.2/tokenizer.model']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPo4ynC6MYx-",
        "outputId": "2106156d-0d64-41d7-d3bd-ff0f5e6c41e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!cp -r /content/drive/MyDrive/chatgpt/models/models/Undi95_Xwin-MLewd-13B-V0.2 /content/text-generation-webui-camenduru/models"
      ],
      "id": "HPo4ynC6MYx-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kt3AhFYViZUr"
      },
      "outputs": [],
      "source": [
        "!sudo add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!sudo apt-get update -qq 2>&1 > /dev/null\n",
        "!sudo apt -y install -qq google-drive-ocamlfuse 2>&1 > /dev/null\n",
        "!google-drive-ocamlfuse"
      ],
      "id": "kt3AhFYViZUr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6oS0RwfimHB"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -qq w3m # to act as web browser\n",
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ],
      "id": "W6oS0RwfimHB"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw-Sx5vccHTt",
        "outputId": "744a959f-ef76-4a46-d8ea-735ce0714bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,009 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,202 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,404 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,275 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,430 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,471 kB]\n",
            "Fetched 8,024 kB in 2s (4,153 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.2 [45.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Fetched 1,513 kB in 1s (1,480 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 120882 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Cloning into 'text-generation-webui-camenduru'...\n",
            "remote: Enumerating objects: 13418, done.\u001b[K\n",
            "remote: Counting objects: 100% (1495/1495), done.\u001b[K\n",
            "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
            "remote: Total 13418 (delta 1039), reused 1396 (delta 1012), pack-reused 11923\u001b[K\n",
            "Receiving objects: 100% (13418/13418), 29.72 MiB | 13.78 MiB/s, done.\n",
            "Resolving deltas: 100% (8977/8977), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install --yes aria2\n",
        "!apt-get -y install -qq aria2\n",
        "!git clone https://github.com/mrRobot95/text-generation-webui-camenduru"
      ],
      "id": "sw-Sx5vccHTt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aad93961-cd30-4db2-b6c9-e64cc60946a7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cd /content\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install --yes aria2\n",
        "!apt-get -y install -qq aria2\n",
        "!git clone https://github.com/mrRobot95/text_gen\n",
        "%cd /content/text-gen\n",
        "\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-7B-uncensored-GPTQ/raw/main/config.json -d /content/text-generation-webui-camenduru/models/WizardLM-7B-uncensored-GPTQ -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-7B-uncensored-GPTQ/raw/main/generation_config.json -d /content/text-generation-webui-camenduru/models/WizardLM-7B-uncensored-GPTQ -o generation_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-7B-uncensored-GPTQ/raw/main/special_tokens_map.json -d /content/text-generation-webui-camenduru/models/WizardLM-7B-uncensored-GPTQ -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-7B-uncensored-GPTQ/resolve/main/tokenizer.model -d /content/text-generation-webui-camenduru/models/WizardLM-7B-uncensored-GPTQ -o tokenizer.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-7B-uncensored-GPTQ/raw/main/tokenizer_config.json -d /content/text-generation-webui-camenduru/models/WizardLM-7B-uncensored-GPTQ -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/4bit/WizardLM-7B-uncensored-GPTQ/resolve/main/WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors -d /content/text-generation-webui-camenduru/models/WizardLM-7B-uncensored-GPTQ -o WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors\n",
        "\n",
        "'''\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/raw/main/config.json -d /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g -o config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/raw/main/generation_config.json -d /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g -o generation_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/raw/main/special_tokens_map.json -d /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g -o special_tokens_map.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/resolve/main/tokenizer.model -d /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g -o tokenizer.model\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/raw/main/tokenizer_config.json -d /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g -o tokenizer_config.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ/resolve/main/model.safetensors -d /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g -o 4bit-128g.safetensors\n",
        "'''\n",
        "!echo \"dark_theme: true\" > /content/settings.yaml\n",
        "!echo \"chat_style: wpp\" >> /content/settings.yaml"
      ],
      "id": "aad93961-cd30-4db2-b6c9-e64cc60946a7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLHT1auIZq4t",
        "outputId": "f21acea4-5f80-44e8-daa8-7fe41473cc0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.0.tar.gz (718 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.7/718.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-7.0.0-py3-none-any.whl size=21129 sha256=0ce3e1b24ea3f7a0d39bfba476285ad99fdeab965af521d1d31088978992ac80\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/29/7b/f64332aa7e5e88fbd56d4002185ae22dcdc83b35b3d1c2cbf5\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.0\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/chatgpt/models/openAI-13B.q8_0.gguf /content/text_gen/models\n",
        "!pip install pyngrok"
      ],
      "id": "rLHT1auIZq4t"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eusi-563mX0i",
        "outputId": "76da6979-3854-468e-f056-93f4ea06297c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xformers\n",
            "  Downloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl (211.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->xformers) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->xformers) (1.3.0)\n",
            "Installing collected packages: xformers\n",
            "Successfully installed xformers-0.0.22.post7\n"
          ]
        }
      ],
      "source": [
        "!pip install xformers"
      ],
      "id": "Eusi-563mX0i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swXP-Y__7pJg",
        "outputId": "7b1020b9-8a65-4fa6-f02c-8ab19655bf78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: server.py [-h] [--notebook] [--chat] [--multi-user] [--character CHARACTER] [--model MODEL]\n",
            "                 [--lora LORA [LORA ...]] [--model-dir MODEL_DIR] [--lora-dir LORA_DIR]\n",
            "                 [--model-menu] [--no-stream] [--settings SETTINGS]\n",
            "                 [--extensions EXTENSIONS [EXTENSIONS ...]] [--verbose] [--chat-buttons]\n",
            "                 [--loader LOADER] [--cpu] [--auto-devices]\n",
            "                 [--gpu-memory GPU_MEMORY [GPU_MEMORY ...]] [--cpu-memory CPU_MEMORY] [--disk]\n",
            "                 [--disk-cache-dir DISK_CACHE_DIR] [--load-in-8bit] [--bf16] [--no-cache]\n",
            "                 [--xformers] [--sdp-attention] [--trust-remote-code] [--use_fast]\n",
            "                 [--load-in-4bit] [--compute_dtype COMPUTE_DTYPE] [--quant_type QUANT_TYPE]\n",
            "                 [--use_double_quant] [--threads THREADS] [--threads-batch THREADS_BATCH]\n",
            "                 [--n_batch N_BATCH] [--no-mmap] [--mlock] [--mul_mat_q]\n",
            "                 [--cache-capacity CACHE_CAPACITY] [--n-gpu-layers N_GPU_LAYERS]\n",
            "                 [--tensor_split TENSOR_SPLIT] [--n_ctx N_CTX] [--llama_cpp_seed LLAMA_CPP_SEED]\n",
            "                 [--numa] [--wbits WBITS] [--model_type MODEL_TYPE] [--groupsize GROUPSIZE]\n",
            "                 [--pre_layer PRE_LAYER [PRE_LAYER ...]] [--checkpoint CHECKPOINT]\n",
            "                 [--monkey-patch] [--triton] [--no_inject_fused_attention] [--no_inject_fused_mlp]\n",
            "                 [--no_use_cuda_fp16] [--desc_act] [--disable_exllama] [--gpu-split GPU_SPLIT]\n",
            "                 [--max_seq_len MAX_SEQ_LEN] [--cfg-cache] [--deepspeed]\n",
            "                 [--nvme-offload-dir NVME_OFFLOAD_DIR] [--local_rank LOCAL_RANK]\n",
            "                 [--rwkv-strategy RWKV_STRATEGY] [--rwkv-cuda-on] [--alpha_value ALPHA_VALUE]\n",
            "                 [--rope_freq_base ROPE_FREQ_BASE] [--compress_pos_emb COMPRESS_POS_EMB]\n",
            "                 [--listen] [--listen-host LISTEN_HOST] [--listen-port LISTEN_PORT] [--share]\n",
            "                 [--auto-launch] [--gradio-auth GRADIO_AUTH] [--gradio-auth-path GRADIO_AUTH_PATH]\n",
            "                 [--ssl-keyfile SSL_KEYFILE] [--ssl-certfile SSL_CERTFILE] [--api]\n",
            "                 [--api-blocking-port API_BLOCKING_PORT] [--api-streaming-port API_STREAMING_PORT]\n",
            "                 [--public-api] [--public-api-id PUBLIC_API_ID]\n",
            "                 [--multimodal-pipeline MULTIMODAL_PIPELINE]\n",
            "\n",
            "options:\n",
            "  -h, --help                                 show this help message and exit\n",
            "  --notebook                                 DEPRECATED\n",
            "  --chat                                     DEPRECATED\n",
            "  --multi-user                               Multi-user mode. Chat histories are not saved or\n",
            "                                             automatically loaded. WARNING: this is highly\n",
            "                                             experimental.\n",
            "  --character CHARACTER                      The name of the character to load in chat mode by\n",
            "                                             default.\n",
            "  --model MODEL                              Name of the model to load by default.\n",
            "  --lora LORA [LORA ...]                     The list of LoRAs to load. If you want to load more\n",
            "                                             than one LoRA, write the names separated by spaces.\n",
            "  --model-dir MODEL_DIR                      Path to directory with all the models\n",
            "  --lora-dir LORA_DIR                        Path to directory with all the loras\n",
            "  --model-menu                               Show a model menu in the terminal when the web UI is\n",
            "                                             first launched.\n",
            "  --no-stream                                DEPRECATED\n",
            "  --settings SETTINGS                        Load the default interface settings from this yaml\n",
            "                                             file. See settings-template.yaml for an example. If\n",
            "                                             you create a file called settings.yaml, this file\n",
            "                                             will be loaded by default without the need to use the\n",
            "                                             --settings flag.\n",
            "  --extensions EXTENSIONS [EXTENSIONS ...]   The list of extensions to load. If you want to load\n",
            "                                             more than one extension, write the names separated by\n",
            "                                             spaces.\n",
            "  --verbose                                  Print the prompts to the terminal.\n",
            "  --chat-buttons                             Show buttons on chat tab instead of hover menu.\n",
            "  --loader LOADER                            Choose the model loader manually, otherwise, it will\n",
            "                                             get autodetected. Valid options: transformers,\n",
            "                                             autogptq, gptq-for-llama, exllama, exllama_hf,\n",
            "                                             llamacpp, rwkv\n",
            "  --cpu                                      Use the CPU to generate text. Warning: Training on\n",
            "                                             CPU is extremely slow.\n",
            "  --auto-devices                             Automatically split the model across the available\n",
            "                                             GPU(s) and CPU.\n",
            "  --gpu-memory GPU_MEMORY [GPU_MEMORY ...]   Maximum GPU memory in GiB to be allocated per GPU.\n",
            "                                             Example: --gpu-memory 10 for a single GPU, --gpu-\n",
            "                                             memory 10 5 for two GPUs. You can also set values in\n",
            "                                             MiB like --gpu-memory 3500MiB.\n",
            "  --cpu-memory CPU_MEMORY                    Maximum CPU memory in GiB to allocate for offloaded\n",
            "                                             weights. Same as above.\n",
            "  --disk                                     If the model is too large for your GPU(s) and CPU\n",
            "                                             combined, send the remaining layers to the disk.\n",
            "  --disk-cache-dir DISK_CACHE_DIR            Directory to save the disk cache to. Defaults to\n",
            "                                             \"cache\".\n",
            "  --load-in-8bit                             Load the model with 8-bit precision (using\n",
            "                                             bitsandbytes).\n",
            "  --bf16                                     Load the model with bfloat16 precision. Requires\n",
            "                                             NVIDIA Ampere GPU.\n",
            "  --no-cache                                 Set use_cache to False while generating text. This\n",
            "                                             reduces the VRAM usage a bit at a performance cost.\n",
            "  --xformers                                 Use xformer's memory efficient attention. This should\n",
            "                                             increase your tokens/s.\n",
            "  --sdp-attention                            Use torch 2.0's sdp attention.\n",
            "  --trust-remote-code                        Set trust_remote_code=True while loading a model.\n",
            "                                             Necessary for ChatGLM and Falcon.\n",
            "  --use_fast                                 Set use_fast=True while loading a tokenizer.\n",
            "  --load-in-4bit                             Load the model with 4-bit precision (using\n",
            "                                             bitsandbytes).\n",
            "  --compute_dtype COMPUTE_DTYPE              compute dtype for 4-bit. Valid options: bfloat16,\n",
            "                                             float16, float32.\n",
            "  --quant_type QUANT_TYPE                    quant_type for 4-bit. Valid options: nf4, fp4.\n",
            "  --use_double_quant                         use_double_quant for 4-bit.\n",
            "  --threads THREADS                          Number of threads to use.\n",
            "  --threads-batch THREADS_BATCH              Number of threads to use for batches/prompt\n",
            "                                             processing.\n",
            "  --n_batch N_BATCH                          Maximum number of prompt tokens to batch together\n",
            "                                             when calling llama_eval.\n",
            "  --no-mmap                                  Prevent mmap from being used.\n",
            "  --mlock                                    Force the system to keep the model in RAM.\n",
            "  --mul_mat_q                                Activate new mulmat kernels.\n",
            "  --cache-capacity CACHE_CAPACITY            Maximum cache capacity. Examples: 2000MiB, 2GiB. When\n",
            "                                             provided without units, bytes will be assumed.\n",
            "  --n-gpu-layers N_GPU_LAYERS                Number of layers to offload to the GPU.\n",
            "  --tensor_split TENSOR_SPLIT                Split the model across multiple GPUs, comma-separated\n",
            "                                             list of proportions, e.g. 18,17\n",
            "  --n_ctx N_CTX                              Size of the prompt context.\n",
            "  --llama_cpp_seed LLAMA_CPP_SEED            Seed for llama-cpp models. Default 0 (random)\n",
            "  --numa                                     Activate NUMA task allocation for llama.cpp\n",
            "  --wbits WBITS                              Load a pre-quantized model with specified precision\n",
            "                                             in bits. 2, 3, 4 and 8 are supported.\n",
            "  --model_type MODEL_TYPE                    Model type of pre-quantized model. Currently LLaMA,\n",
            "                                             OPT, and GPT-J are supported.\n",
            "  --groupsize GROUPSIZE                      Group size.\n",
            "  --pre_layer PRE_LAYER [PRE_LAYER ...]      The number of layers to allocate to the GPU. Setting\n",
            "                                             this parameter enables CPU offloading for 4-bit\n",
            "                                             models. For multi-gpu, write the numbers separated by\n",
            "                                             spaces, eg --pre_layer 30 60.\n",
            "  --checkpoint CHECKPOINT                    The path to the quantized checkpoint file. If not\n",
            "                                             specified, it will be automatically detected.\n",
            "  --monkey-patch                             Apply the monkey patch for using LoRAs with quantized\n",
            "                                             models.\n",
            "  --triton                                   Use triton.\n",
            "  --no_inject_fused_attention                Do not use fused attention (lowers VRAM\n",
            "                                             requirements).\n",
            "  --no_inject_fused_mlp                      Triton mode only: Do not use fused MLP (lowers VRAM\n",
            "                                             requirements).\n",
            "  --no_use_cuda_fp16                         This can make models faster on some systems.\n",
            "  --desc_act                                 For models that don't have a quantize_config.json,\n",
            "                                             this parameter is used to define whether to set\n",
            "                                             desc_act or not in BaseQuantizeConfig.\n",
            "  --disable_exllama                          Disable ExLlama kernel, which can improve inference\n",
            "                                             speed on some systems.\n",
            "  --gpu-split GPU_SPLIT                      Comma-separated list of VRAM (in GB) to use per GPU\n",
            "                                             device for model layers, e.g. 20,7,7\n",
            "  --max_seq_len MAX_SEQ_LEN                  Maximum sequence length.\n",
            "  --cfg-cache                                ExLlama_HF: Create an additional cache for CFG\n",
            "                                             negative prompts. Necessary to use CFG with that\n",
            "                                             loader, but not necessary for CFG with base ExLlama.\n",
            "  --deepspeed                                Enable the use of DeepSpeed ZeRO-3 for inference via\n",
            "                                             the Transformers integration.\n",
            "  --nvme-offload-dir NVME_OFFLOAD_DIR        DeepSpeed: Directory to use for ZeRO-3 NVME\n",
            "                                             offloading.\n",
            "  --local_rank LOCAL_RANK                    DeepSpeed: Optional argument for distributed setups.\n",
            "  --rwkv-strategy RWKV_STRATEGY              RWKV: The strategy to use while loading the model.\n",
            "                                             Examples: \"cpu fp32\", \"cuda fp16\", \"cuda fp16i8\".\n",
            "  --rwkv-cuda-on                             RWKV: Compile the CUDA kernel for better performance.\n",
            "  --alpha_value ALPHA_VALUE                  Positional embeddings alpha factor for NTK RoPE\n",
            "                                             scaling. Use either this or compress_pos_emb, not\n",
            "                                             both.\n",
            "  --rope_freq_base ROPE_FREQ_BASE            If greater than 0, will be used instead of\n",
            "                                             alpha_value. Those two are related by rope_freq_base\n",
            "                                             = 10000 * alpha_value ^ (64 / 63).\n",
            "  --compress_pos_emb COMPRESS_POS_EMB        Positional embeddings compression factor. Should be\n",
            "                                             set to (context length) / (model's original context\n",
            "                                             length). Equal to 1/rope_freq_scale.\n",
            "  --listen                                   Make the web UI reachable from your local network.\n",
            "  --listen-host LISTEN_HOST                  The hostname that the server will use.\n",
            "  --listen-port LISTEN_PORT                  The listening port that the server will use.\n",
            "  --share                                    Create a public URL. This is useful for running the\n",
            "                                             web UI on Google Colab or similar.\n",
            "  --auto-launch                              Open the web UI in the default browser upon launch.\n",
            "  --gradio-auth GRADIO_AUTH                  set gradio authentication like \"username:password\";\n",
            "                                             or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\"\n",
            "  --gradio-auth-path GRADIO_AUTH_PATH        Set the gradio authentication file path. The file\n",
            "                                             should contain one or more user:password pairs in\n",
            "                                             this format: \"u1:p1,u2:p2,u3:p3\"\n",
            "  --ssl-keyfile SSL_KEYFILE                  The path to the SSL certificate key file.\n",
            "  --ssl-certfile SSL_CERTFILE                The path to the SSL certificate cert file.\n",
            "  --api                                      Enable the API extension.\n",
            "  --api-blocking-port API_BLOCKING_PORT      The listening port for the blocking API.\n",
            "  --api-streaming-port API_STREAMING_PORT    The listening port for the streaming API.\n",
            "  --public-api                               Create a public URL for the API using Cloudfare.\n",
            "  --public-api-id PUBLIC_API_ID              Tunnel ID for named Cloudflare Tunnel. Use together\n",
            "                                             with public-api option.\n",
            "  --multimodal-pipeline MULTIMODAL_PIPELINE  The multimodal pipeline to use. Examples: llava-7b,\n",
            "                                             llava-13b.\n"
          ]
        }
      ],
      "source": [
        "!python server.py --help"
      ],
      "id": "swXP-Y__7pJg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74b4e58b-a3cd-4b63-818d-9c11632d7177",
        "outputId": "84274a27-570d-4f5b-f06e-5b94613b15b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-generation-webui-camenduru\n",
            "2023-11-15 06:14:29 WARNING:\u001b[33mThe gradio \"share link\" feature uses a proprietary executable to create a reverse tunnel. Use it with care.\u001b[0m\n",
            "2023-11-15 06:14:29 WARNING:\u001b[33m\n",
            "You are potentially exposing the web UI to the entire internet without any access password.\n",
            "You can create one with the \"--gradio-auth\" flag like this:\n",
            "\n",
            "--gradio-auth username:password\n",
            "\n",
            "Make sure to replace username:password with your own.\u001b[0m\n",
            "2023-11-15 06:14:39 WARNING:\u001b[33mWARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
            "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
            "    Python  3.10.13 (you have 3.10.12)\n",
            "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
            "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
            "  Set XFORMERS_MORE_DETAILS=1 for more details\u001b[0m\n",
            "2023-11-15 06:14:42 INFO:\u001b[32mLoading the extension \"ngrok\"...\u001b[0m\n",
            "installing pyngrok ...\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.0.1.tar.gz (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.8/731.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-7.0.1-py3-none-any.whl size=21141 sha256=a9e471cb061abd0d66c5835deb64fb603feeb550222c7572395f8d3823a19878\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/32/0e/27789b6fde02bf2b320d6f1a0fd9e1354b257c5f75eefc29bc\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.0.1\n",
            "pyngrok installed successfully!\n",
            "2023-11-15 06:14:49 WARNING:\u001b[33mt=2023-11-15T06:14:49+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\u001b[0m\n",
            "https://2b47-35-232-40-121.ngrok-free.app\n",
            "2023-11-15 06:14:49 INFO:\u001b[32mLoading the extension \"gallery\"...\u001b[0m\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://29687103f4056faefa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "2023-11-15 06:17:02 INFO:\u001b[32mLoading Undi95_Xwin-MLewd-13B-V0.2...\u001b[0m\n",
            "2023-11-15 06:17:02 INFO:\u001b[32mUsing the following 4-bit params: {'load_in_4bit': True, 'bnb_4bit_compute_dtype': torch.float16, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': False}\u001b[0m\n",
            "Loading checkpoint shards: 100% 3/3 [02:02<00:00, 40.83s/it]\n",
            "2023-11-15 06:19:06 INFO:\u001b[32mReplaced attention with xformers_attention\u001b[0m\n",
            "2023-11-15 06:19:06 INFO:\u001b[32mLoaded the model in 123.72 seconds.\n",
            "\u001b[0m\n",
            "2023-11-15 06:19:50.250217: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-15 06:19:50.250270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-15 06:19:50.250307: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-15 06:19:52.471629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Output generated in 15.19 seconds (1.32 tokens/s, 20 tokens, context 67, seed 1141138957)\n",
            "Output generated in 22.66 seconds (8.78 tokens/s, 199 tokens, context 117, seed 523449951)\n",
            "Output generated in 12.91 seconds (7.67 tokens/s, 99 tokens, context 316, seed 1544416984)\n",
            "Output generated in 26.34 seconds (7.55 tokens/s, 199 tokens, context 434, seed 1909358367)\n",
            "Output generated in 25.01 seconds (7.96 tokens/s, 199 tokens, context 633, seed 777185829)\n",
            "Output generated in 16.14 seconds (7.25 tokens/s, 117 tokens, context 832, seed 800261516)\n",
            "Output generated in 24.97 seconds (7.97 tokens/s, 199 tokens, context 980, seed 416278864)\n",
            "Output generated in 27.71 seconds (7.18 tokens/s, 199 tokens, context 1179, seed 1211080036)\n",
            "Output generated in 6.58 seconds (4.71 tokens/s, 31 tokens, context 1466, seed 1964080929)\n",
            "Output generated in 30.81 seconds (6.46 tokens/s, 199 tokens, context 1541, seed 1261470431)\n",
            "Output generated in 31.55 seconds (6.31 tokens/s, 199 tokens, context 1739, seed 298947113)\n",
            "Output generated in 29.39 seconds (6.43 tokens/s, 189 tokens, context 1950, seed 4215939)\n",
            "Output generated in 36.66 seconds (5.43 tokens/s, 199 tokens, context 2153, seed 866491422)\n",
            "Output generated in 27.16 seconds (5.19 tokens/s, 141 tokens, context 2379, seed 2116714511)\n",
            "Output generated in 37.76 seconds (5.27 tokens/s, 199 tokens, context 2529, seed 1072418847)\n",
            "Output generated in 18.45 seconds (3.20 tokens/s, 59 tokens, context 2737, seed 70730239)\n",
            "Output generated in 16.87 seconds (4.15 tokens/s, 70 tokens, context 2806, seed 1899455262)\n",
            "Output generated in 19.41 seconds (2.88 tokens/s, 56 tokens, context 2893, seed 254740223)\n",
            "Output generated in 9.77 seconds (2.25 tokens/s, 22 tokens, context 2966, seed 1510038082)\n",
            "Output generated in 12.58 seconds (0.56 tokens/s, 7 tokens, context 2995, seed 589067716)\n",
            "Output generated in 43.50 seconds (4.58 tokens/s, 199 tokens, context 3015, seed 683837602)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui-camenduru/modules/callbacks.py\", line 56, in gentask\n",
            "    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n",
            "  File \"/content/text-generation-webui-camenduru/modules/text_generation.py\", line 349, in generate_with_callback\n",
            "    shared.model.generate(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1652, in generate\n",
            "    return self.sample(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2734, in sample\n",
            "    outputs = self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1038, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 925, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 389, in forward\n",
            "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 576.81 MiB is free. Process 84277 has 14.18 GiB memory in use. Of the allocated memory 12.29 GiB is allocated by PyTorch, and 903.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Output generated in 7.56 seconds (0.00 tokens/s, 0 tokens, context 3238, seed 1336786270)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui-camenduru/modules/callbacks.py\", line 56, in gentask\n",
            "    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n",
            "  File \"/content/text-generation-webui-camenduru/modules/text_generation.py\", line 349, in generate_with_callback\n",
            "    shared.model.generate(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1652, in generate\n",
            "    return self.sample(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2734, in sample\n",
            "    outputs = self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1038, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 925, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 389, in forward\n",
            "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 802.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 562.81 MiB is free. Process 84277 has 14.20 GiB memory in use. Of the allocated memory 12.30 GiB is allocated by PyTorch, and 908.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Output generated in 7.52 seconds (0.00 tokens/s, 0 tokens, context 3242, seed 393263654)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/text-generation-webui-camenduru/modules/callbacks.py\", line 56, in gentask\n",
            "    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n",
            "  File \"/content/text-generation-webui-camenduru/modules/text_generation.py\", line 349, in generate_with_callback\n",
            "    shared.model.generate(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1652, in generate\n",
            "    return self.sample(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2734, in sample\n",
            "    outputs = self(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1038, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 925, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 635, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 389, in forward\n",
            "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 796.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 604.81 MiB is free. Process 84277 has 14.15 GiB memory in use. Of the allocated memory 12.27 GiB is allocated by PyTorch, and 895.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Output generated in 12.92 seconds (0.00 tokens/s, 0 tokens, context 3229, seed 1448373352)\n",
            "2023-11-15 06:55:45 INFO:\u001b[32mLoading Undi95_Xwin-MLewd-13B-V0.2...\u001b[0m\n",
            "Loading checkpoint shards: 100% 3/3 [00:48<00:00, 16.11s/it]\n",
            "2023-11-15 06:56:34 INFO:\u001b[32mReplaced attention with xformers_attention\u001b[0m\n",
            "2023-11-15 06:56:34 INFO:\u001b[32mLoaded the model in 49.40 seconds.\n",
            "\u001b[0m\n",
            "2023-11-15 08:57:39 INFO:\u001b[32mLoading Undi95_Xwin-MLewd-13B-V0.2...\u001b[0m\n",
            "Loading checkpoint shards: 100% 3/3 [00:47<00:00, 15.97s/it]\n",
            "2023-11-15 08:58:27 INFO:\u001b[32mReplaced attention with xformers_attention\u001b[0m\n",
            "2023-11-15 08:58:27 INFO:\u001b[32mLoaded the model in 48.64 seconds.\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#%%capture captured_output\n",
        "\n",
        "%cd /content/text-generation-webui-camenduru\n",
        "\n",
        "!python server.py --share --extensions ngrok --loader llama --wbits 4 --groupsize 128 --auto-devices --xformers --n-gpu-layers 39 --model-dir /content #--model-dir /content/drive/MyDrive/chatgpt/models/models\n",
        "#--model /content/text-gen/models/HuggingFaceH4_zephyr-7b-beta\n",
        "#/content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g\n",
        "#--model /content/text-gen/models/vicuna-v1.1-13b-GPTQ-4bit-128g"
      ],
      "id": "74b4e58b-a3cd-4b63-818d-9c11632d7177"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xurGfCxWlcA",
        "outputId": "20e5e243-5a1f-4a35-ea8d-d1fa12ab201d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-generation-webui-camenduru\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.0/853.0 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.1/396.1 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.1/311.1 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.6/248.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.5/227.5 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.2/220.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-grammar (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.6.0 which is incompatible.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 59.6.0 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\n",
            "seaborn 0.12.2 requires numpy!=1.24.0,>=1.17, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%cd /content/text-generation-webui-camenduru\n",
        "!pip install -q -r requirements.txt"
      ],
      "id": "1xurGfCxWlcA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9949b9f2-0ef5-412a-99db-3ad3863cfffc",
        "outputId": "075bf386-7717-49de-f38d-6fc94f6c1539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "python3: can't open file '/content/text-gen/repositories/flash-attention/setup.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python /content/text-gen/repositories/flash-attention/setup.py install\n"
      ],
      "id": "9949b9f2-0ef5-412a-99db-3ad3863cfffc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "599ef920-e011-4f68-9db7-3b9dd5c6c257"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from IPython.utils import capture\n",
        "huggingface_org=\"Undi95\"\n",
        "huggingface_repo=\"Xwin-MLewd-13B-V0.2\"\n",
        "huggingface_branch=\"main\"\n",
        "model_name=\"Xwin-MLewd-13B-V0.2\"\n",
        "%cd /content/text-gen/models\n",
        "#![[ ! -f models/$model_name/config.json ]] && python download-model.py $huggingface_org/$huggingface_repo --branch $huggingface_branch -q --progress && echo \"Download completed.\"\n",
        "\n",
        "if not os.path.isfile(f\"models/{model_name}/config.json\"):\n",
        "    with capture.capture_output() as captured:\n",
        "        !python download-model.py {huggingface_org}/{huggingface_repo} --branch {huggingface_branch}\n",
        "    print(\"Download completed.\")\n"
      ],
      "id": "599ef920-e011-4f68-9db7-3b9dd5c6c257"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f129cf26-d862-44ed-81cf-525f131295e6"
      },
      "outputs": [],
      "source": [
        "cmd = f\"python server.py --loader exllama --model {model_name} --settings settings-colab.json {' '.join(params)} --load-in-4bit --public-api\"\n",
        "print(cmd)\n",
        "#!$cmd"
      ],
      "id": "f129cf26-d862-44ed-81cf-525f131295e6"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}